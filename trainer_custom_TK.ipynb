{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from script.tokenizer import KmerTokenizer\n",
    "\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, PreTrainedTokenizerFast, LineByLineTextDataset, BertConfig, AutoTokenizer, pipeline\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/TeleoSplit/train.csv')\n",
    "val_df = pd.read_csv('Data/TeleoSplit/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>phylum</th>\n",
       "      <th>class</th>\n",
       "      <th>order</th>\n",
       "      <th>family</th>\n",
       "      <th>genus</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCCCAAACAATAAACACACGAAACTAACTAAAATGCTTCGAACCGT...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Scombriformes</td>\n",
       "      <td>Gempylidae</td>\n",
       "      <td>Promethichthys</td>\n",
       "      <td>Promethichthysprometheus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCCCAAGTTCAATATATCCTTCTAACTAAGAAGTTAGCCGAACAAA...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Salmoniformes</td>\n",
       "      <td>Salmonidae</td>\n",
       "      <td>Oncorhynchus</td>\n",
       "      <td>Oncorhynchusmasou</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCCCGAACTTAACCCACGAACCTTACCTAAACTGTTTTACATGAAA...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Holocentriformes</td>\n",
       "      <td>Holocentridae</td>\n",
       "      <td>Neoniphon</td>\n",
       "      <td>Neoniphonargenteus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCTGTTAAACAGCAACCAATGTAAATAACACAAAAGCACCAACG...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Cypriniformes</td>\n",
       "      <td>Gastromyzontidae</td>\n",
       "      <td>Yaoshania</td>\n",
       "      <td>Yaoshaniapachychilus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCCCAAGCTTCCGGCCCTAATTAATTAAAACCCTACAACTGCAAAG...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Istiophoriformes</td>\n",
       "      <td>Istiophoridae</td>\n",
       "      <td>Istiophorus</td>\n",
       "      <td>Istiophorusplatypterus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15693</th>\n",
       "      <td>CCCCTGTCAAAATGCAATAAAGATACTTAACACCAAAGCACTGACA...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Cypriniformes</td>\n",
       "      <td>Leuciscidae</td>\n",
       "      <td>Gila</td>\n",
       "      <td>Gilaelegans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15694</th>\n",
       "      <td>CATCAAGTCATAACTTTACACAGTTAACAGACAGATGGGAAAAGTC...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Cypriniformes</td>\n",
       "      <td>Danionidae</td>\n",
       "      <td>Microdevario</td>\n",
       "      <td>Microdevarionanus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15695</th>\n",
       "      <td>CCCCTTATACCTTAAAATTTTAACAAAATTACTAATACCTTAACCT...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Siluriformes</td>\n",
       "      <td>Pimelodidae</td>\n",
       "      <td>Pseudoplatystoma</td>\n",
       "      <td>Pseudoplatystomacorruscans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15696</th>\n",
       "      <td>CCTCAAAAATATTCACCCCTTCTATAAACATACTTCTCCAATAAGA...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Chondrichthyes</td>\n",
       "      <td>Carcharhiniformes</td>\n",
       "      <td>Carcharhinidae</td>\n",
       "      <td>Glyphis</td>\n",
       "      <td>Glyphisgarricki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15697</th>\n",
       "      <td>CTCCGAATGAACCCTTGATGATACCTAATATGTTTTACCAATTAAG...</td>\n",
       "      <td>Chordata</td>\n",
       "      <td>Actinopteri</td>\n",
       "      <td>Gadiformes</td>\n",
       "      <td>Macrouridae</td>\n",
       "      <td>Trachyrincus</td>\n",
       "      <td>Trachyrincusmurrayi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15698 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sequence    phylum  \\\n",
       "0      CCCCAAACAATAAACACACGAAACTAACTAAAATGCTTCGAACCGT...  Chordata   \n",
       "1      CCCCAAGTTCAATATATCCTTCTAACTAAGAAGTTAGCCGAACAAA...  Chordata   \n",
       "2      CCCCGAACTTAACCCACGAACCTTACCTAAACTGTTTTACATGAAA...  Chordata   \n",
       "3      CCCCTGTTAAACAGCAACCAATGTAAATAACACAAAAGCACCAACG...  Chordata   \n",
       "4      CCCCAAGCTTCCGGCCCTAATTAATTAAAACCCTACAACTGCAAAG...  Chordata   \n",
       "...                                                  ...       ...   \n",
       "15693  CCCCTGTCAAAATGCAATAAAGATACTTAACACCAAAGCACTGACA...  Chordata   \n",
       "15694  CATCAAGTCATAACTTTACACAGTTAACAGACAGATGGGAAAAGTC...  Chordata   \n",
       "15695  CCCCTTATACCTTAAAATTTTAACAAAATTACTAATACCTTAACCT...  Chordata   \n",
       "15696  CCTCAAAAATATTCACCCCTTCTATAAACATACTTCTCCAATAAGA...  Chordata   \n",
       "15697  CTCCGAATGAACCCTTGATGATACCTAATATGTTTTACCAATTAAG...  Chordata   \n",
       "\n",
       "                class              order            family             genus  \\\n",
       "0         Actinopteri      Scombriformes        Gempylidae    Promethichthys   \n",
       "1         Actinopteri      Salmoniformes        Salmonidae      Oncorhynchus   \n",
       "2         Actinopteri   Holocentriformes     Holocentridae         Neoniphon   \n",
       "3         Actinopteri      Cypriniformes  Gastromyzontidae         Yaoshania   \n",
       "4         Actinopteri   Istiophoriformes     Istiophoridae       Istiophorus   \n",
       "...               ...                ...               ...               ...   \n",
       "15693     Actinopteri      Cypriniformes       Leuciscidae              Gila   \n",
       "15694     Actinopteri      Cypriniformes        Danionidae      Microdevario   \n",
       "15695     Actinopteri       Siluriformes       Pimelodidae  Pseudoplatystoma   \n",
       "15696  Chondrichthyes  Carcharhiniformes    Carcharhinidae           Glyphis   \n",
       "15697     Actinopteri         Gadiformes       Macrouridae      Trachyrincus   \n",
       "\n",
       "                          species  \n",
       "0        Promethichthysprometheus  \n",
       "1               Oncorhynchusmasou  \n",
       "2              Neoniphonargenteus  \n",
       "3            Yaoshaniapachychilus  \n",
       "4          Istiophorusplatypterus  \n",
       "...                           ...  \n",
       "15693                 Gilaelegans  \n",
       "15694           Microdevarionanus  \n",
       "15695  Pseudoplatystomacorruscans  \n",
       "15696             Glyphisgarricki  \n",
       "15697         Trachyrincusmurrayi  \n",
       "\n",
       "[15698 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_double(df):\n",
    "    poisson_noDoublon=pd.DataFrame()\n",
    "\n",
    "    for seq in tqdm(df['sequence'].unique()):\n",
    "\n",
    "        sub=df[df['sequence']==seq]\n",
    "\n",
    "\n",
    "\n",
    "        poisson_noDoublon=pd.concat([poisson_noDoublon,sub.sample(1)])\n",
    "    return poisson_noDoublon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6303/6303 [00:04<00:00, 1287.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df=no_double(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 563/563 [00:00<00:00, 4038.76it/s]\n"
     ]
    }
   ],
   "source": [
    "val_df=no_double(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ambigus={'Y':['C','T'],\n",
    "               'W':['A','T'],\n",
    "               'M':['A','C'],\n",
    "               'K':['T','G'],\n",
    "               'R':['A','G'],\n",
    "               'S':['C','G'],\n",
    "\n",
    "               'B':['C','T','G'],\n",
    "               'V':['A','C','G'],\n",
    "               'H':['A','C','T'],\n",
    "               'D':['A','G','T'],\n",
    "               \n",
    "               'X':['A','C','G','T'],\n",
    "               'N':['A','C','G','T'],\n",
    "               ' ':['']\n",
    "               }\n",
    "def clean_nucleotide(df):\n",
    "    c=0\n",
    "    df_copy=df.copy()\n",
    "    sequence=np.array(df['sequence'].values)\n",
    "    #print(len(sequence))\n",
    "    for anbigous_nuc in dict_ambigus.keys():\n",
    "        #print(anbigous_nuc)\n",
    "        for i, seq in enumerate(sequence) :\n",
    "            #print(\"okok\")\n",
    "\n",
    "        \n",
    "            #print(lambda m:random.choice(dict_ambigus[anbigous_nuc]))\n",
    "            c+=len(re.findall(seq,anbigous_nuc))\n",
    "            sequence[i]=re.sub(anbigous_nuc, lambda m:random.choice(dict_ambigus[anbigous_nuc]) , seq)\n",
    "    df_copy['sequence']=sequence\n",
    "    print(c)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "train_df=clean_nucleotide(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "val_df=clean_nucleotide(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RATIO_UNBALANCED = 0.2\n",
    "\n",
    "MIN_LEN = 0.9\n",
    "\n",
    "FREQ_CROP = 0.90\n",
    "\n",
    "LOW_NUC = 1/40\n",
    "\n",
    "HIGHT_NUC = 1/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_mutation={'C':['T','G','A'],'T':['C','G','A'],'G':['T','C','A'],'A':['T','G','C']}\n",
    "\n",
    "def chunk(seq,crop=True,crop_freq=1-FREQ_CROP,min_lenght=MIN_LEN):\n",
    "    \n",
    "    if random.random()>=crop_freq:\n",
    "\n",
    "        sub_lenght=random.randint(int(len(seq)*MIN_LEN),len(seq))\n",
    "\n",
    "        start_index=random.randint(0,len(seq)-sub_lenght)\n",
    "\n",
    "        sub_seq=seq[start_index:start_index+sub_lenght]\n",
    "\n",
    "        return sub_seq\n",
    "    \n",
    "    else : return seq\n",
    "\n",
    "    \n",
    "def mutationv0(seq,mutation_ratio_min=LOW_NUC,mutation_ration_max=HIGHT_NUC):\n",
    "    \n",
    "    lenght=len(seq)\n",
    "    \n",
    "    mutation_ration=random.uniform(mutation_ratio_min,mutation_ration_max)\n",
    "\n",
    "    index_sub=range(lenght)\n",
    "\n",
    "    index_mutation=random.sample(index_sub,int(mutation_ration*lenght))\n",
    "\n",
    "    for ind in index_mutation:\n",
    "\n",
    "        new_nuc=random.choice(dict_mutation[seq[ind]])\n",
    "\n",
    "        seq=seq[:ind]+new_nuc+seq[ind+1:]\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_data(df,save_file= 'augment.csv',func_mutation=mutationv0, func_chunk=chunk ,sample_par_class=None, colunm_aug='family'):\n",
    "\n",
    "    df_manage=df.copy()\n",
    "\n",
    "    df_manage['synthetic']=[0]*df.shape[0]\n",
    "\n",
    "    df_manage.to_csv(save_file,index=False)\n",
    "    \n",
    "    with open(save_file, mode='a') as csv_file:\n",
    "\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=df_manage.columns,lineterminator='\\n')\n",
    "\n",
    "        count=df_manage[colunm_aug].value_counts().to_frame()\n",
    "\n",
    "        count_dict=count.to_dict()\n",
    "\n",
    "        if sample_par_class ==None:\n",
    "            max_sample=count.reset_index()['count'][0]\n",
    "            sample_par_class=int(max_sample*RATIO_UNBALANCED)\n",
    "\n",
    "        sub_df_need_augment=df_manage[df_manage[colunm_aug].map(df_manage[colunm_aug].value_counts()) < sample_par_class]\n",
    "        label_need_augment=sub_df_need_augment[colunm_aug].unique()\n",
    "        #print(sub_df_need_augment)\n",
    "        #print(label_need_augment)\n",
    "        for lab in tqdm(label_need_augment):\n",
    "            \n",
    "\n",
    "            sub_df=sub_df_need_augment[sub_df_need_augment[colunm_aug]==lab]\n",
    "            \n",
    "            augment_number=sample_par_class-count_dict[\"count\"][lab]\n",
    "            #print(sub_df)\n",
    "            #print(augment_number)\n",
    "            for i in range(augment_number):\n",
    "\n",
    "\n",
    "                row=sub_df.sample(1)\n",
    "                #print(row.shape)\n",
    "\n",
    "                for index, r in row.iterrows():\n",
    "\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    #print(sequence)\n",
    "                    r['sequence'] = func_mutation(func_chunk(r['sequence']))\n",
    "                    #print(label,sequence)\n",
    "                    \n",
    "                    r['synthetic']=1\n",
    "                        \n",
    "                    \n",
    "                    writer.writerow(r.to_dict())\n",
    "\n",
    "        csv_file.close() \n",
    "\n",
    "    return pd.read_csv(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 366/366 [00:03<00:00, 96.96it/s] \n"
     ]
    }
   ],
   "source": [
    "train_df=augment_data(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207/207 [00:00<00:00, 1200.16it/s]\n"
     ]
    }
   ],
   "source": [
    "val_df=augment_data(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeN=KmerTokenizer(4, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokeN(train_df['sequence'].tolist(), truncation=True, max_length=512)\n",
    "val_encodings = tokeN(val_df['sequence'].tolist(), truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the combined data\n",
    "label_encoder.fit(pd.concat([train_df['family'], val_df['family']]))\n",
    "\n",
    "# Encode the labels\n",
    "train_labels = label_encoder.transform(train_df['family'])\n",
    "val_labels = label_encoder.transform(val_df['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': train_labels\n",
    "})\n",
    "\n",
    "val_dataset = Dataset.from_dict({\n",
    "    'input_ids': val_encodings['input_ids'],\n",
    "    'attention_mask': val_encodings['attention_mask'],\n",
    "    'labels': val_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionnaries for the class labels\n",
    "id2label = {i: label for i, label in enumerate(label_encoder.classes_)}\n",
    "label2id = {label: i for i, label in enumerate(label_encoder.classes_)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokeN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Auguste\\.cache\\huggingface\\modules\\transformers_modules\\zhihan1996\\DNABERT-2-117M\\d064dece8a8b41d9fb8729fbe3435278786931f1\\bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(\"zhihan1996/DNABERT-2-117M\", \n",
    "                                    num_labels=len(id2label), \n",
    "                                    max_position_embeddings=514,\n",
    "                                    id2label=id2label,\n",
    "                                    label2id=label2id\n",
    ")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"zhihan1996/DNABERT-2-117M\", trust_remote_code=True, ignore_mismatched_sizes=True, config=config)\n",
    "\n",
    "# Verif that the model is correctly configured\n",
    "model.id2label = id2label\n",
    "model.label2id = label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import IntervalStrategy\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test2\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    #auto_find_batch_size=True,\n",
    "    num_train_epochs=20,\n",
    "    logging_steps=75,\n",
    "    eval_on_start=True,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    eval_accumulation_steps=1,\n",
    "    logging_first_step=True\n",
    ")\n",
    "\n",
    "# class CustomTrainer(Trainer):\n",
    "#     def _evaluate(self, trial=None, ignore_keys_for_eval=None):\n",
    "#         # Implement your evaluation logic here\n",
    "#         # For example, calculate accuracy on a validation set\n",
    "#         metrics = {\"accuracy\": 0.9}  # Placeholder for actual evaluation logic\n",
    "#         return metrics\n",
    "\n",
    "# Il y a avait un bug avec le compute metrics donc enlevÃ© pour le moment\n",
    "\n",
    "#def compute_metrics(eval_pred):\n",
    " #   predictions, labels = eval_pred\n",
    "  #  predictions = np.argmax(predictions, axis=1)\n",
    "   # return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokeN,\n",
    "    data_collator=data_collator,\n",
    "    #compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Update the trainer arguments to use your datasets\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_model\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=15,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",  # Use F1 score to select the best model\n",
    "    greater_is_better=True,      # Higher F1 score is better\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# Load F1 metric\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred.predictions, eval_pred.label_ids\n",
    "\n",
    "    # Add debugging information to inspect shapes\n",
    "    print(f\"Predictions type: {type(predictions)}\")\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    print(f\"Predictions shape after handling tuple: {predictions.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "\n",
    "    # Ensure predictions are 2D\n",
    "    if predictions.ndim == 2:\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    elif predictions.ndim > 2:\n",
    "        raise ValueError(f\"Unexpected predictions shape: {predictions.shape}\")\n",
    "\n",
    "    return f1_metric.compute(predictions=predictions, references=labels, average='macro')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokeN,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # Include the compute_metrics function\n",
    "    #class_weights=class_weights_tensor.to(\"cuda\")  # Uncomment and define this line if you're using class weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34c6ff64b40144da8e4836ceb614cf37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35940 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 5.9074, 'grad_norm': 2.4107718467712402, 'learning_rate': 9.860879243183083e-06, 'epoch': 0.21}\n",
      "{'loss': 5.7811, 'grad_norm': 3.810699939727783, 'learning_rate': 9.721758486366167e-06, 'epoch': 0.42}\n",
      "{'loss': 5.6186, 'grad_norm': 3.016672372817993, 'learning_rate': 9.582637729549249e-06, 'epoch': 0.63}\n",
      "{'loss': 5.4458, 'grad_norm': 3.546008348464966, 'learning_rate': 9.443516972732332e-06, 'epoch': 0.83}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6ac315a4a54a21976b443b0d9a472f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions type: <class 'tuple'>\n",
      "Predictions shape after handling tuple: (1664, 378)\n",
      "Labels shape: (1664,)\n",
      "{'eval_loss': 5.292776107788086, 'eval_f1': 0.12000339558283843, 'eval_runtime': 10.321, 'eval_samples_per_second': 161.225, 'eval_steps_per_second': 10.077, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "KmerTokenizer.save_vocabulary() got an unexpected keyword argument 'filename_prefix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1933\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   1934\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1935\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   1936\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1937\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:2365\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2365\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[0;32m   2367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[0;32m   2368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[0;32m   2369\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:2796\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2793\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[0;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n\u001b[0;32m   2797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:2875\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2873\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_dir(trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[0;32m   2874\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(run_dir, checkpoint_folder)\n\u001b[1;32m-> 2875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_model(output_dir, _internal_call\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   2877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_only_model:\n\u001b[0;32m   2878\u001b[0m     \u001b[38;5;66;03m# Save optimizer and scheduler\u001b[39;00m\n\u001b[0;32m   2879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_optimizer_and_scheduler(output_dir)\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:3429\u001b[0m, in \u001b[0;36mTrainer.save_model\u001b[1;34m(self, output_dir, _internal_call)\u001b[0m\n\u001b[0;32m   3426\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\u001b[38;5;241m.\u001b[39msave_checkpoint(output_dir)\n\u001b[0;32m   3428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 3429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save(output_dir)\n\u001b[0;32m   3431\u001b[0m \u001b[38;5;66;03m# Push to the Hub when `save_model` is called by the user.\u001b[39;00m\n\u001b[0;32m   3432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpush_to_hub \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _internal_call:\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\trainer.py:3505\u001b[0m, in \u001b[0;36mTrainer._save\u001b[1;34m(self, output_dir, state_dict)\u001b[0m\n\u001b[0;32m   3500\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave_pretrained(\n\u001b[0;32m   3501\u001b[0m         output_dir, state_dict\u001b[38;5;241m=\u001b[39mstate_dict, safe_serialization\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_safetensors\n\u001b[0;32m   3502\u001b[0m     )\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 3505\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(output_dir)\n\u001b[0;32m   3507\u001b[0m \u001b[38;5;66;03m# Good practice: save your training arguments together with the trained model\u001b[39;00m\n\u001b[0;32m   3508\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, TRAINING_ARGS_NAME))\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2587\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.save_pretrained\u001b[1;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[0;32m   2583\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpecial tokens file saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspecial_tokens_map_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2585\u001b[0m file_names \u001b[38;5;241m=\u001b[39m (tokenizer_config_file, special_tokens_map_file)\n\u001b[1;32m-> 2587\u001b[0m save_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_pretrained(\n\u001b[0;32m   2588\u001b[0m     save_directory\u001b[38;5;241m=\u001b[39msave_directory,\n\u001b[0;32m   2589\u001b[0m     file_names\u001b[38;5;241m=\u001b[39mfile_names,\n\u001b[0;32m   2590\u001b[0m     legacy_format\u001b[38;5;241m=\u001b[39mlegacy_format,\n\u001b[0;32m   2591\u001b[0m     filename_prefix\u001b[38;5;241m=\u001b[39mfilename_prefix,\n\u001b[0;32m   2592\u001b[0m )\n\u001b[0;32m   2594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push_to_hub:\n\u001b[0;32m   2595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upload_modified_files(\n\u001b[0;32m   2596\u001b[0m         save_directory,\n\u001b[0;32m   2597\u001b[0m         repo_id,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2600\u001b[0m         token\u001b[38;5;241m=\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   2601\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Auguste\\anaconda3\\envs\\torch\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2636\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._save_pretrained\u001b[1;34m(self, save_directory, file_names, legacy_format, filename_prefix)\u001b[0m\n\u001b[0;32m   2633\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(out_str)\n\u001b[0;32m   2634\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded tokens file saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madded_tokens_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2636\u001b[0m vocab_files \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_vocabulary(save_directory, filename_prefix\u001b[38;5;241m=\u001b[39mfilename_prefix)\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file_names \u001b[38;5;241m+\u001b[39m vocab_files \u001b[38;5;241m+\u001b[39m (added_tokens_file,)\n",
      "\u001b[1;31mTypeError\u001b[0m: KmerTokenizer.save_vocabulary() got an unexpected keyword argument 'filename_prefix'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
